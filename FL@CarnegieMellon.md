Back to https://github.com/Kwangkee/FL
***


## Virginia Smith
https://scholar.google.com/citations?hl=ko&user=bldHpWIAAAAJ&view_op=list_works&sortby=pubdate

## Gauri Joshi
https://scholar.google.com/citations?hl=ko&user=yqIoH34AAAAJ&view_op=list_works&sortby=pubdate

## Yae Jee Cho
https://scholar.google.com/citations?hl=ko&user=MR333jsAAAAJ&view_op=list_works&sortby=pubdate  
https://yaejeec.github.io/   

- [[Towards Understanding Biased Client Selection in Federated Learning](https://github.com/Kwangkee/FL/blob/main/FL@CarnegieMellon.md#biased-client-selection)], https://scholar.google.com/scholar?hl=ko&as_sdt=0%2C5&q=Towards+Understanding+Biased+Client+Selection+in+Federated+Learning&btnG=  
- To Federate or Not To Federate: Incentivizing Client Participation in Federated Learning, https://scholar.google.com/citations?view_op=view_citation&hl=ko&user=MR333jsAAAAJ&sortby=pubdate&citation_for_view=MR333jsAAAAJ:UebtZRa9Y70C  
- Client Selection in Federated Learning: Convergence Analysis and Power-of-Choice Selection Strategies, https://arxiv.org/abs/2010.01243

## Biased Client Selection
Towards Understanding Biased Client Selection in Federated Learning, https://scholar.google.com/scholar?hl=ko&as_sdt=0%2C5&q=Towards+Understanding+Biased+Client+Selection+in+Federated+Learning&btnG=  

>In our work, we present the convergence analysis of federated learning with biased client selection and quantify how the bias affects convergence speed. **We show that biasing client selection towards clients with higher local loss yields faster error convergence.** From this insight, we propose Power-of-Choice, a communication- and computation-efficient client selection framework that flexibly spans the trade-off between convergence speed and solution bias. Extensive experiments demonstrate that Power-of-Choice can converge up to 3× faster and give 10% higher test accuracy than the baseline random selection.
>**Client Selection Aware of Local Loss.** Adaptive client selection that is cognizant of the training progress of clients is not yet well-understood. Such biased client selection strategies can accelerate error convergence in heterogeneous environments by preferentially selecting clients with higher local loss values, as we show in this paper. 
>Our Contributions. In this paper, we present the
first convergence analysis of FL with biased client selection that is cognizant of the training progress at
each client. We prove theoretically that biasing client
selection towards clients with higher local losses increases the rate of convergence compared to unbiased
client selection. Using this insight, we propose the
Power-of-Choice client selection strategy and show
that Power-of-Choice yields up to 3× faster convergence with 10% higher test performance than the standard federated averaging with random selection. We
also propose communication and computation efficient
variants of Power-of-Choice that incur minimal additional resource overhead. In fact, we show that even
with 3× less clients participating in each round as compared to random selection, Power-of-Choice gives
2× faster convergence and 5% higher test accuracy.







***
Back to the [Top](#list)  
Back to https://github.com/Kwangkee/FL
